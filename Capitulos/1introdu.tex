\section{Motivação}
A busca por otimização de código é cada vez mais importante no contexto de sistemas embarcados, nos quais os recursos computacionais são extremamente limitados \cite{hong2025automated}. Os microcontroladores, ou MCUs, que são o cérebro destes sistemas e os responsáveis por executar tarefas específicas de acordo com a programação definida, contam com menos memória e menor capacidade de processamento quando comparados aos processadores usados em computadores \textit{desktops} ou \textit{notebooks}. Um exemplo são os modelos de microcontroladores da família STM32 \cite{stm32f103r8-datasheet}, amplamente utilizados na indústria, que apresentam memórias SRAM variando entre 2 e 620 KB e podem operar com frequência de até 600 MHz. Cada família de microcontroladores apresenta um conjunto de registradores, periféricos e instruções da CPU, as quais devem ser consideradas no processo de otimização.

A otimização de código consiste em aplicar técnicas, ou transformações, que melhoram o desempenho ou reduzem o tamanho de um programa sem alterar sua funcionalidade original \cite{maslov_code_nodate}. A eficácia de cada transformação pode variar conforme o programa e a arquitetura de hardware considerada \cite{wang_machine_2018}. Por exemplo, um microcontrolador AVR ATmega328P, arquitetura RISC de 8 \textit{bits}, possui um conjunto reduzido de 131 instruções, em sua maioria executando em um ciclo de \textit{clock} e recursos de memória bem modestos: apenas 2 KB de RAM e 32 KB de memória \textit{flash} \cite{noauthor_avr1632dd1420_2024}. Ao contrário, um processador moderno como o Intel Core i7-1165G7, arquitetura CISC de 64 \textit{bits}, suporta centenas de instruções e conta com memórias \textit{flashes} e execução fora de ordem para alto desempenho \cite{noauthor_intel_nodate}. Essas diferenças de arquitetura impactam a geração de código intermediário (\textit{Intermediate Representation}): uma operação que resulta em uma única instrução complexa no Core i7 pode requerer várias instruções mais simples no AVR, influenciando o tamanho do código gerado e o nível de otimização possível em cada caso.
%duvida na referencia da intel neste parágrafo, no ano da referência, eu coloco o ano que eu ví a página ou posso deixar sem ano ? A nota veio pro overleaf sem ano de lançamento, somente com a data da URL.

% Prof. Thiago: A data de lançamento é uma boa data para o year da referência. Além dela vai aparecer o "acessado em:".

Para simplificar o uso dos MCUs, os fabricantes fornecem bibliotecas de funções e rotinas, geralmente nomeadas como  \textit{Hardware Abstraction Layers} (HALs). Como cada HAL é específico do fabricante, a diversidade de periféricos e a falta de padronização fazem com que as interfaces de programação específicas não sejam compatíveis entre si. Como resultado, sempre que um novo modelo de microcontrolador é adotado em um sistema embarcado, os programadores precisam aprender sua interface de programação específica, dificultando ainda mais a otimização de código. Neste sentido, foram propostas ao longo do tempo, bibliotecas para linguagem de programação de propósito geral e \textit{frameworks}. Esses \textit{frameworks} atuam como uma camada de \textit{software} intermediária, oferecendo funções que vão além das camadas HAL disponibilizadas pelos fabricantes. Um exemplo bastante conhecido é o Arduino \cite{Arduino}, que abstrai o acesso aos periféricos mais utilizados em plataformas como AVR, STM32 e ExpressIf32. Outras ferramentas similares são o CMSIS\footnote{Site do CMSIS: \url{https://www.arm.com/technologies/cmsis}}, MBED\footnote{Site do MBED: \url{https://os.mbed.com/handbook/mbed-Microcontrollers}}, STM32CUBE\footnote{Site do STM32CUBE: \url{https://www.st.com/en/ecosystems/stm32cube.html}} e LibOpenCM3\footnote{Site do LibOpenCM3: \url{https://libopencm3.org}}, sendo essas últimas especificamente voltadas para arquiteturas baseadas em ARM.


Tendo a otimização de código um papel fundamental na realização do potencial máximo de \textit{software} e \textit{hardware}, desenvolvedores buscam uma solução universal para transformar programas de entrada em versões semanticamente equivalentes, porém mais eficientes, sem esforço manual. A fim de alcançar este objetivo, os compiladores utilizam um \textit{front-end}, que traduz o código-fonte de uma linguagem de programação para uma representação intermediária (IR), um \textit{middle-end optimizer}, que executa otimizações independentes da linguagem e da plataforma sobre a IR do código original, e um \textit{back-end}, que converte a IR em código binário. O otimizador é comumente implementado como uma sequência de passes que aplicam transformações no código. O desempenho da otimização depende principalmente da seleção e ordem das passagens de otimização \cite{deng_compilerdream_2024}.

Os compiladores disponibilizam estratégias de otimização com ordens pré-definidas, cada uma estabelecendo um conjunto ordenado de passes de otimização, como -O1, -O2 e -O3 para melhorar a velocidade de execução, e -Os e -Oz\footnote{-Oz existe somente na família de compiladores do LLVM.} para reduzir o tamanho do programa. Porém, com a diversidade de programas e plataformas, essas estratégias pré-definidas acabam não sendo ideais em todos os casos, e é possível melhorar o desempenho de um programa específico se encontrarmos uma sequência de passes melhor, comparada com os passes fornecidos pelos compiladores \cite{georgiou2018less,cummins_llm_2025, deng_compilerdream_2024}.


%  Atualmente, na educação, a robótica é vista como potencializadora do trabalho coletivo, permitindo que os estudantes programem, discutam e consequentemente, aprendam brincando \cite{parreira_robotica_2022}. 


Na última década, no  trabalho de \citeonline{wang_machine_2018},  foi empregado inteligência artificial para otimização de código por meio de aprendizado de máquina, utilizando atributos como tamanho das funções e/ou quantidade de blocos básicos, definidos com base no conhecimento humano, para representar o programa de forma que um modelo de aprendizado de máquina conseguisse operar sobre ele. Outras abordagens usaram redes neurais gráficas \cite{liang_learning_2023}. Contudo, segundo \citeonline{cummins_llm_2025}, tais pesquisas têm um problema em comum: elas não conseguem representar o programa original por completo, o que interfere no aprendizado e resultado do modelo de IA. Modelos de IA projetados para representação textual, como ChatGPT\footnote{Site de introdução do ChatGPT: \url{https://openai.com/index/chatgpt/}}, \textit{Code Llama}\footnote{Site da Meta Code Llama: \url{https://www.llama.com/code-llama}} e Codex\footnote{Site do Codex: \url{https://openai.com/index/openai-codex}}, avançam no entendimento estatístico de códigos e podem sugerir conclusões prováveis para códigos incompletos. No entanto, os mesmos não foram treinados de forma específica para otimização de código. O ChatGPT, por exemplo, consegue fazer pequenos ajustes em programas, como marcar variáveis para serem armazenadas como registradores e até tenta otimizações mais substanciais como vetorização. No entanto, ele com frequência alucina e comete erros, resultando muitas vezes em códigos incorretos \cite{cummins_llm_2025}.


Recentemente, a elaboração de \textit{Large Language Models} (LLMs) para otimização de código tem mostrado resultados promissores, como evidenciado por \citeonline{cummins_llm_2025}, que apresentou o modelo LLM Compiler, treinado com aprendizado supervisionado, especificamente para selecionar passes de otimização no nível \texttt{LLVM-IR}, alcançando uma redução média de 3{\,}\% a 5{\,}\% na contagem de instruções em comparação à otimização padrão do compilador LLVM com a flag -Oz. Além disso, os autores disponibilizaram os modelos publicamente na plataforma \textit{Hugging Face}\footnote{Site do \textit{Hugging face: \url{https://huggingface.co}}}, sob uma licença comercial personalizada, com o objetivo de incentivar ampla reutilização, permitindo assim que pesquisadores acadêmicos e profissionais da indústria possam expandir e aprofundar pesquisas nessa área emergente.

O \textit{Hugging Face} destaca-se como uma plataforma central no atual ecossistema de Inteligência Artificial, atuando como um repositório massivo de modelos de linguagem e outros artefatos de \textit{machine learning}. O \textit{Hugging Face Hub} hospeda hoje mais de 900 mil modelos de IA, além de centenas de milhares de conjuntos de dados e aplicativos de demonstração, todos de acesso aberto, facilitando a colaboração e a reutilização pela comunidade. Essa plataforma cumpre um papel de distribuição e compartilhamento de modelos: pesquisadores e desenvolvedores podem publicar seus modelos treinados, permitindo que outros os utilizem ou aprimorem sem precisar treiná-los do zero. Isso democratiza o acesso a  modelos avançados de linguagem e incentiva uma abordagem comunitária na evolução da IA, em contraste a soluções proprietárias restritas a grandes empresas \cite{noauthor_hugging_nodate1}.


% Muito detalhe para uma introdução, comentei. Essa tarefa, no entanto, é complexa, sendo importante notar que um passe(ou transformação) de código nem sempre é uma otimização. Em vez disso, ela pode modificar a estrutura da Representação Intermediária (IR) para permitir a aplicação de outras otimizações. Assim, identificar combinações eficientes de passes requer um nível de análise que os LLMs mais recentes tem demonstrado alcançar, de acordo com os resultados apresentados por \citeonline{cummins_large_2023}. Portanto, um código menor não significa necessariamente ser mais rápido em sua execução, mas pode abrir espaços para outras otimizações.\cite{georgiou_less_2018}.

O \textit{LLM Compiler} \citeonline{cummins_llm_2025} é voltado à otimização de compiladores, porém focado em arquiteturas de CPUs de propósito geral (principalmente desktop/servers x86-64 e ARM de 64 bits) não abrangendo microcontroladores de 8 ou 16 bits. O modelo foi treinado com conjuntos de dados de 	LLVM-IR e código de montagem dessas plataformas de alto desempenho. Assim, surge a necessidade de customizar o modelo, de forma a torná-lo assertivo para microcontroladores (MCUs) como o STM32, considerando suas restrições arquiteturais (baixos clocks na ordem de dezenas de MHz e pouquíssimos kilobytes de memória disponíveis). Um \textit{LLM Compiler} especializado para MCUs poderia ser treinado para ``entender'' as peculiaridades do código em arquiteturas de 8/16 bits e otimizar visando tamanho e eficiência energética, explorando instruções e padrões específicos desses processadores de recursos limitados. Em outras palavras, o \textit{LLM Compiler} proposto por \citeonline{cummins_llm_2025} mostrou o potencial dos modelos de linguagem na otimização de código para CPUs poderosas e é possível estender essa abordagem aos microcontroladores, refinando o treinamento do modelo.

Neste sentido, os projetos ``\textit{Robotics Language}: Uma Linguagem de Programação de Propósito Específico para Microcontroladores'' (PI05974-2024) e seu antecessor ``Especificação e Construção de Protótipos Funcionais de Kits Robóticos de Baixo Custo para uso em Processos de Ensino-Aprendizagem'' (PI02361-2018) \cite{oliveira2024robcmp}, ambos desenvolvidos na Universidade Federal de Jataí, têm como foco o desenvolvimento de uma linguagem de programação voltada especificamente para aplicações em robótica e microcontroladores, denominada \textit{The Robotics Language} (ROBL), juntamente com seu compilador correspondente, o Robcmp, com o fim de criar um ecossistema de robótica educacional de baixo custo.

\section{Objetivo do Trabalho}
Portanto, este trabalho tem como objetivo refinar o modelo \textit{LLM Compiler} no contexto de MCUs, especificamente o STM32F103C8T6, usando como base programas escritos na \textit{Robotics Language}.A proposta busca adaptar técnicas de otimização já consolidadas para arquiteturas de propósito geral à realidade de sistemas embarcados com recursos limitados, ainda pouco explorada na literatura.  Os objetivos específicos foram:
\begin{enumerate}
    \item Converter repositórios de código escritos em C para a ROBL, ignorando os códigos em C que necessitam de funcionalidades inexistentes na ROBL;
    \item Compilar o repositório convertido usando otimizações pré-determinadas e busca semi-exaustiva de passes, para encontrar o melhor código otimizado de cada programa;
    \item Criar, para treinamento supervisionado do modelo, pares \textit{prompt-label} com o código em ROBL e o melhor código otimizado encontrado; e
    \item Executar experimentos para validar o desempenho do modelo refinado ao indicar sequências de passes para otimização de programas não usados no treinamento, escritos em ROBL.
\end{enumerate}

\section{Contribuição do Trabalho}
Como contribuição, desenvolveu-se um conjunto de \textit{scripts} de preparação de dados, conversão de código, \textit{autotuning} e avaliação quantitativa, que geram datasets de treinamento para modelos LLMs. Os datasets podem ser utilizados para refinar ou treinar modelos, com o objetivo de produzir sequências de passes de otimização a partir de programas fonte de entrada, visando gerar versões de código objeto menores. 

%A relevância da pesquisa se justifica pela crescente demanda por eficiência em ambientes restritos além da oportunidade de inovar ao aplicar modelos de linguagem de grande escala na otimização automatizada de código para microcontroladores.

% Alinhando-se ao trabalho proposto por \citeonline{cummins_llm_2025}, que demonstra a possibilidade de se aprender heurísticas diretamente do código-fonte por meio de aprendizado profundo, estudando o potencial dessas técnicas no contexto de sistemas embarcados, eliminando a necessidade de heurísticas manuais e abrindo caminho para soluções mais generalizáveis.


