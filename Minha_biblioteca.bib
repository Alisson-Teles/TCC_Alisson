
@misc{cummins_large_2023,
	title = {Large {Language} {Models} for {Compiler} {Optimization}},
	url = {http://arxiv.org/abs/2309.07062},
	doi = {10.48550/arXiv.2309.07062},
	abstract = {We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0\% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91\% of the time and perfectly emulating the output of the compiler 70\% of the time.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Elhoushi, Mostafa and Liang, Youwei and Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Hazelwood, Kim and Synnaeve, Gabriel and Leather, Hugh},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {Full Text PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\R32AHCVR\\Cummins et al. - 2023 - Large Language Models for Compiler Optimization.pdf:application/pdf;Snapshot:C\:\\Users\\Alisson Teles\\Zotero\\storage\\RCWX4CJ9\\2309.html:text/html},
}

@inproceedings{parallelProgram,
  title={Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces},
  author={Wei, Anjiang and Nie, Allen and Teixeira, Thiago SFX and Yadav, Rohan and Lee, Wonchan and Wang, Ke and Aiken, Alex},
  year = {2025},
  booktitle={Forty-second International Conference on Machine Learning}
}

@article{santana_cirurgia_2022,
	title = {Cirurgia {Robótica} no {Brasil}},
	volume = {11},
	copyright = {Copyright (c) 2022},
	issn = {2525-3409},
	url = {https://rsdjournal.org/index.php/rsd/article/view/33223},
	doi = {10.33448/rsd-v11i12.33223},
	abstract = {Objective: To understand the context of Robotic Surgery in Brazil. Methodology: Research carried out in Pubmed, Scielo and Virtual Health Library databases using descriptors verified in DeCS. We found 88 results and, of these, 12 articles were selected to compose the results of this review. Results: Robotic surgery is a rapidly evolving technology and is highly effective, but it is still growing in our country, as is the certification of surgeons to act in this way. According to the Brazilian College of Surgeons, the acquisition of specific knowledge and skills for the surgeon to achieve proficiency before performing surgical procedures on humans is essential. In the meantime, there is an educational incentive coming from this institution, proposing certification that integrates training and performance evaluation, since the creation of a regulation for qualification in robotic surgery should encourage Brazilian hospitals to apply objective qualification criteria for this type of procedure, in the sense of qualifying assistance. This is because it has control of the operative field, high resolution image in third dimension (3D), freedom of movement of the instruments, reduction of tremors, greater autonomy of the surgeon with less use of auxiliaries and, mainly, precision. Thus, it is highly advantageous in relation to surgeries such as laparotomy and videolaparoscopy. Conclusion: Robotic surgery is effective in complex and minimally invasive surgeries, in addition to having an advantage over several surgical modalities, due to the low risk of complications. However, certification and education to carry it out should still be encouraged in the country.},
	language = {pt},
	number = {12},
	urldate = {2025-04-02},
	journal = {Research, Society and Development},
	author = {Santana, Bárbara Reis de and Teixeira, Larissa de Araújo Correia and Monteiro, Marina Schuster and Lima, Sônia Oliveira},
	month = sep,
	year = {2022},
	note = {Number: 12},
	keywords = {Brazil.},
	pages = {e138111233223--e138111233223},
	file = {Full Text PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\77XBNF36\\Santana et al. - 2022 - Cirurgia Robótica no Brasil.pdf:application/pdf},
}

@article{parreira_robotica_2022,
	title = {{ROBÓTICA} {NA} {EDUCAÇÃO}: {UMA} {REVISÃO} {DA} {LITERATURA}},
	volume = {10},
	copyright = {Copyright (c) 2022 Ulisses Queiroz Parreira, Deive Barbosa Alves, Marcos Antonio de Sousa},
	issn = {2318-6674},
	shorttitle = {{ROBÓTICA} {NA} {EDUCAÇÃO}},
	url = {https://periodicoscientificos.ufmt.br/ojs/index.php/reamec/article/view/12976},
	doi = {10.26571/reamec.v10i1.12976},
	abstract = {This article aims to present a bibliographical review based on academic theses and dissertations that allude to robotics in education, defended in the period between February 2016 and March 2021, in two important bases, namely: the Brazilian Digital Library of Theses and Dissertations (BDTD) and the CAPES Theses and Dissertations Catalog (CDT). With this investigation, it was intended to find, in addition to advances in this theme, the way in which robotics has been used in the educational context. For the feasibility of this review, guiding questions were used, in addition to inclusion and exclusion criteria for the selection of manuscripts. As an impact of this research, there is a discreet amount of investigations related to robotics in the classroom, the accumulation of these researches, as pointed out in previous analyses, still remains in the Northeast, South and Southeast regions of Brazil. Despite different approaches, it is noted that such researches always mention this theme as a strategy, tool, or even something to be inserted between different curricular components, always aiming to improve the teaching-learning process. In addition, we also identified that it is seen as an enhancer of collective work, allowing students to program, discuss and, consequently, learn by playing.},
	language = {pt},
	number = {1},
	urldate = {2025-04-02},
	journal = {REAMEC - Rede Amazônica de Educação em Ciências e Matemática},
	author = {Parreira, Ulisses Queiroz and Alves, Deive Barbosa and Sousa, Marcos Antonio de},
	month = jan,
	year = {2022},
	note = {Number: 1},
	keywords = {Educação, Educación, Education, Educational Robotics, Literature review, Revisão de Literatura, Revisión de literatura, Robótica Educacional, Robótica Educativa},
	pages = {e22005--e22005},
	file = {Full Text PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\MG7NRN4S\\Parreira et al. - 2022 - ROBÓTICA NA EDUCAÇÃO UMA REVISÃO DA LITERATURA.pdf:application/pdf},
}

@inproceedings{cummins_llm_2025,
	address = {Las Vegas NV USA},
	title = {{LLM} {Compiler}: {Foundation} {Language} {Models} for {Compiler} {Optimization}},
	isbn = {979-8-4007-1407-8},
	shorttitle = {{LLM} {Compiler}},
	url = {https://dl.acm.org/doi/10.1145/3708493.3712691},
	doi = {10.1145/3708493.3712691},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {International} {Conference} on {Compiler} {Construction}},
	publisher = {ACM},
	author = {Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh},
	month = feb,
	year = {2025},
	pages = {141--153},
	file = {Full Text PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\MHD5BZN5\\Cummins et al. - 2025 - LLM Compiler Foundation Language Models for Compiler Optimization.pdf:application/pdf},
}

@inproceedings{deng_compilerdream_2024,
  title={CompilerDream: Learning a Compiler World Model for General Code Optimization},
  author={Deng, Chaoyi and Wu, Jialong and Feng, Ningya and Wang, Jianmin and Long, Mingsheng},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2},
  pages={486--497},
  year={2025}
  }

@inproceedings{georgiou2018less,
  author = {Georgiou, Kyriakos and Blackmore, Craig and Xavier-de-Souza, Samuel and Eder, Kerstin},
  title = {Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption},
  year = {2018},
  isbn = {9781450357807},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3207719.3207727},
  doi = {10.1145/3207719.3207727},
  abstract = {This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -02, while preserving their original ordering, significant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two different versions of the LLVM compilation framework; v3.8 and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4\% and 5.3\% was achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with execution time improvements ranging from 1\% up to 90\% over the -02. The savings that can be achieved are in the same range as what can be achieved by the state-of-the-art compilation approaches that use iterative compilation or machine learning to select flags or to determine phase orderings that result in more efficient code. In contrast to these time consuming and expensive to apply techniques, our approach only needs to test a limited number of optimization configurations, less than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution time, energy consumption and code size at the same time.},
  booktitle = {Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems},
  pages = {35–42},
  numpages = {8},
  keywords = {Autotuning, compiler optimizations, embedded systems, energy consumption, execution time, phase-ordering},
  location = {Sankt Goar, Germany},
  series = {SCOPES '18}
}

@article{wang_machine_2018,
author={Wang, Zheng and O’Boyle, Michael},
  journal={Proceedings of the IEEE}, 
  title={Machine Learning in Compiler Optimization}, 
  year={2018},
  volume={106},
  number={11},
  pages={1879-1901},
  keywords={Machine learning;Optimization;Program processors;Feature extraction;Training data;Data models;High performance computing;Code optimization;compiler;machine learning;program tuning},
  doi={10.1109/JPROC.2018.2817118}
  }

@book{cummins_end--end_2017,
	title = {End-to-{End} {Deep} {Learning} of {Optimization} {Heuristics}},
	abstract = {Accurate automatic optimization heuristics are necessary for dealing with the complexity and diversity of modern hardware and software. Machine learning is a proven technique for learning such heuristics, but its success is bound by the quality of the features used. These features must be hand crafted by developers through a combination of expert domain knowledge and trial and error. This makes the quality of the final model directly dependent on the skill and available time of the system architect. Our work introduces a better way for building heuristics. We develop a deep neural network that learns heuristics over raw code, entirely without using code features. The neural network simultaneously constructs appropriate representations of the code and learns how best to optimize, removing the need for manual feature creation. Further, we show that our neural nets can transfer learning from one optimization problem to another, improving the accuracy of new models, without the help of human experts. We compare the effectiveness of our automatically generated heuristics again stones with features hand-picked by experts. We examine two challenging tasks: predicting optimal mapping for heterogeneous parallelism and GPU thread coarsening factors. In 89\% of the cases, the quality of our fully automatic heuristics matches or surpasses that of state-of-the-art predictive models using hand-crafted features, providing on average 14\% and 12\% more performance with no human effort expended on designing features.},
	author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	month = sep,
	year = {2017},
	doi = {10.1109/PACT.2017.24},
	note = {Pages: 232},
	file = {Versão aceita:C\:\\Users\\Alisson Teles\\Zotero\\storage\\K7LEUGP8\\Cummins et al. - 2017 - End-to-End Deep Learning of Optimization Heuristics.pdf:application/pdf},
}

@misc{liang_learning_2023,
  title={Learning compiler pass orders using coreset and normalized value prediction},
  author={Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others},
  booktitle={International Conference on Machine Learning},
  pages={20746--20762},
  year={2023},
  organization={PMLR}
}

@misc{arduino,
	title = {Arduino},
	url = {https://www.arduino.cc/},
	abstract = {Open-source electronic prototyping platform enabling users to create interactive electronic objects.},
	language = {en},
        year = {2025},
	urldate = {2025-04-22},
	file = {Snapshot:C\:\\Users\\Alisson Teles\\Zotero\\storage\\TJY9BHI4\\www.arduino.cc.html:text/html},
}



@misc{noauthor_hugging_nodate1,
	title = {Hugging },
	url = {https://huggingface.co/docs/hub/en/index},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
        year = {2025},
	urldate = {2025-04-22},
	file = {Snapshot:C\:\\Users\\Alisson Teles\\Zotero\\storage\\CVJH9CBR\\index.html:text/html},
}



@article{maslov_code_nodate,
	title = {Code size optimization using the {Intel}® {C}/{C}++ {Compiler}},
	language = {en},
	author = {Maslov, Sergey and Ganesh, Kittur},
	file = {PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\3EZRLQV3\\Maslov e Ganesh - Code size optimization using the Intel® CC++ Compiler.pdf:application/pdf},
    year = {2014}
}



@article{noauthor_avr1632dd1420_2024,
	title = {{AVR16}/{Datasheet}},
	language = {en},
	year = {2024},
	file = {AVR32-16DD20-14-Complete-DataSheet-DS40002413.pdf:C\:\\Users\\Alisson Teles\\Zotero\\storage\\IDSPHGHJ\\2024 - AVR1632DD1420 Complete Datasheet.pdf:application/pdf},
}



@misc{noauthor_intel_nodate,
	title = {Intel®™},
	url = {https://www.intel.com/content/www/us/en/products/sku/208921/intel-core-i71165g7-processor-12m-cache-up-to-4-70-ghz-with-ipu/specifications.html},
    year={2025},
	abstract = {Intel® Core™ i7-1165G7 Processor (12M Cache, up to 4.70 GHz, with IPU) quick reference with specifications, features, and technologies.},
	language = {en},
	urldate = {2025-04-25},
	journal = {Intel},
}
@inproceedings{liang_learning_2023-1,
	title = {Learning {Compiler} {Pass} {Orders} using {Coreset} and {Normalized} {Value} {Prediction}},
	url = {https://proceedings.mlr.press/v202/liang23f.html},
	abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5\% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.},
	language = {en},
	urldate = {2025-05-05},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and Tian, Yuandong},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {20746--20762},
	file = {Full Text PDF:C\:\\Users\\Alisson Teles\\Zotero\\storage\\EKL623ZX\\Liang et al. - 2023 - Learning Compiler Pass Orders using Coreset and Normalized Value Prediction.pdf:application/pdf},
}
@book{fischer2010crafting,
  title={Crafting a Compiler},
  author={Fischer, C.N. and Cytron, R.K. and LeBlanc, R.J.},
  isbn={9780136067054},
  lccn={2009038265},
  series={Crafting a compiler with C},
  url={https://books.google.com.br/books?id=G4Y_AQAAIAAJ},
  year={2010},
  publisher={Addison-Wesley}
}
@book{cooper2012engineering,
  title={Engineering a Compiler},
  author={Cooper, K.D. and Torczon, L.},
  isbn={9780120884780},
  lccn={2011288670},
  series={Morgan Kaufmann},
  url={https://books.google.com.br/books?id=CGTOlAEACAAJ},
  year={2012},
  publisher={Morgan Kaufmann}
}
@inproceedings{foleiss2009scc,
  title={Scc: Um compilador c como ferramenta de ensino de compiladores},
  author={Foleiss, Juliano Henrique and Assun{\c{c}}{\~a}o, Guilherme Puglia and Cruz, EHM and Gon{\c{c}}alves, RAL and Feltrin, V},
  booktitle={WEAC2009-Workshop Educa{\c{c}}{\~a}o em Arquitetura de Computadores},
  pages={15--22},
  year={2009}
}
@book{lopes2014getting,
  title={Getting started with LLVM core libraries},
  author={Lopes, Bruno Cardoso and Auler, Rafael},
  year={2014},
  publisher={Packt Publishing Ltd}
}

@book{ricarte2008introduccao,
  title={Introdu{\c{c}}{\~a}o {\`a} compila{\c{c}}{\~a}o},
  author={Ricarte, Ivan},
  year={2008},
  publisher={Elsevier}
}
@article{blauth2010linguagens,
  title={Linguagens formais e aut{\^o}matos},
  author={Blauth, Paulo},
  journal={Artmed Editora SA},
  year={2010}
}
@article{aho1995compiladores,
  title={Compiladores: Princ{\i}pios, t{\'e}cnicas e ferramentas},
  author={Aho, Alfred V and Sethi, Ravi and Ullman, Jeffrey D},
  journal={LTC, Rio de Janeiro, Brasil},
  pages={219--276},
  year={2007}
}
@article{costacompiladores,
  title={Compiladores},
  author={Costa, Roselene Henrique Pereira and Rodr{\'\i}guez, Alfredo Johnson and Gomes, Analeia and Rosa, Jo{\~a}o Paulo Resende and Monteiro, Leandro Ferreira and dos Santos Teodoro, Simone Elza},
  year={2023}
}
@book{boden2017inteligencia,
  title={Inteligencia artificial},
  author={Boden, Margaret A},
  year={2017},
  publisher={Turner}
}
@article{faceli2021inteligencia,
  title={Intelig{\^e}ncia artificial: uma abordagem de aprendizado de m{\'a}quina},
  author={Faceli, Katti and Lorena, Ana Carolina and Gama, Jo{\~a}o and Almeida, Tiago Agostinho de and Carvalho, Andr{\'e} Carlos Ponce de Leon Ferreira de},
  year={2021}
}
@book{zhou2021machine,
  title={Machine learning},
  author={Zhou, Zhi-Hua},
  year={2021},
  publisher={Springer nature}
}
@inproceedings{faustino2021new,
  title={New optimization sequences for code-size reduction for the LLVM compilation infrastructure},
  author={Faustino, Anderson and Borin, Edson and Pereira, Fernando and N{\'a}poli, Ot{\'a}vio and Ros{\'a}rio, Vanderson},
  booktitle={Proceedings of the 25th Brazilian Symposium on Programming Languages},
  pages={33--40},
  year={2021}
}
@inproceedings{cummins2021programl,
  title={Programl: A graph-based program representation for data flow analysis and compiler optimizations},
  author={Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh},
  booktitle={International Conference on Machine Learning},
  pages={2244--2253},
  year={2021},
  organization={PMLR}
}

@misc{trofin2021mlgo,title	= {MLGO: a Machine Learning Guided Compiler Optimizations Framework},author	= {Mircea Trofin and Yundi Qian and Eugene Brevdo and Zinan Lin and Krzysztof Choromanski and Xinliang David Li},year	= {2021},URL	= {https://arxiv.org/pdf/2101.04808}}
@article{hussain2016programming,
  title={Programming a microcontroller},
  author={Hussain, Altaf and Hammad, Muhammad and Hafeez, Kamran and Zainab, Tabinda},
  journal={Int. J. Comput. Appl},
  volume={155},
  number={5},
  pages={21--26},
  year={2016}
}
@misc{avr16dd14-datasheet,
  author       = {{Microchip Technology Inc.}},
  title        = {{AVR16DD14 - 8-bit AVR Microcontroller with 16 KB Flash and eXtreme Low Power Technology}},
  year         = {2024},
  url          = {https://www.microchip.com/en-us/product/AVR16DD14},
  note         = {Accessed: 2025-05-20}
}
@misc{stm32f103r8-datasheet,
  author       = {{STMicroelectronics}},
  title        = {{STM32F103R8 - ARM Cortex-M3 32-bit MCU with 64 or 128 Kbytes Flash, 72 MHz CPU, motor control, USB and CAN}},
  year         = {2023},
  url          = {https://www.st.com/resource/en/datasheet/stm32f103r8.pdf},
  note         = {Accessed: 2025-05-20}
}
@article{mernik2005and,
  title={When and how to develop domain-specific languages},
  author={Mernik, Marjan and Heering, Jan and Sloane, Anthony M},
  journal={ACM computing surveys (CSUR)},
  volume={37},
  number={4},
  pages={316--344},
  year={2005},
  publisher={ACM New York, NY, USA}
}
@article{subhimajestic,
  title={Majestic: Uma Amplia{\c{c}}{\~a}o de uma Linguagem de Programa{\c{c}}{\~a}o para Rob{\'o}tica Educacional},
  author={Subhi, V{\'\i}tor Almeida},
  year={2019},
}
@misc{oliveira2024robcmp,
  author       = {Oliveira, Thiago Borges de},
  title        = {{Robcmp: The Robotics Compiler}},
  year         = {2024},
  howpublished = {\url{https://github.com/thborges/robcmp/}},
  note         = {Disponível em: \url{https://github.com/thborges/robcmp/}. Acesso em: 20 maio 2025.}
}
@article{muhammad2015supervised,
  title={SUPERVISED MACHINE LEARNING APPROACHES: A SURVEY.},
  author={Muhammad, Iqbal and Yan, Zhu},
  journal={ICTACT Journal on Soft Computing},
  volume={5},
  number={3},
  year={2015}
}
@article{morandin2022artificial,
  title={What is Artificial Intelligence?},
  author={Morand{\'\i}n-Ahuerma, Fabio},
  year={2022}
}
@article{ludermir,
  title={Intelig{\^e}ncia Artificial e Aprendizado de M{\'a}quina: estado atual e tend{\^e}ncias},
  author={Ludermir, Teresa Bernarda},
  journal={Estudos Avan{\c{c}}ados},
  volume={35},
  pages={85--94},
  year={2021},
  publisher={SciELO Brasil}
}
@book{luger2013inteligência,
  title={Intelig{\^e}ncia Artificial},
  author={Luger, G.},
  isbn={9788581435503},
  url={https://books.google.com.br/books?id=UNEKvQEACAAJ},
  year={2013},
  publisher={PEARSON BRASIL}
}
@book{russell2020artificial,
  title={Artificial Intelligence: A Modern Approach},
  author={Russell, S.J. and Russell, S. and Norvig, P.},
  isbn={9780134610993},
  lccn={2019047498},
  series={Pearson series in artificial intelligence},
  url={https://books.google.com.bn/books?id=koFptAEACAAJ},
  year={2020},
  publisher={Pearson}
}
@article{mahesh2020machine,
  title={Machine learning algorithms-a review},
  author={Mahesh, Batta and others},
  journal={International Journal of Science and Research (IJSR).[Internet]},
  volume={9},
  number={1},
  pages={381--386},
  year={2020}
}
@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM transactions on intelligent systems and technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}
@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{grubisic2024compiler,
  title={Compiler generated feedback for large language models},
  author={Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh},
  journal={arXiv preprint arXiv:2403.14714},
  year={2024}
}
@inproceedings{yang2011finding,
  title={Finding and understanding bugs in C compilers},
  author={Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
  booktitle={Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation},
  pages={283--294},
  year={2011}
}
@inproceedings{da2021anghabench,
  title={Anghabench: A suite with one million compilable c benchmarks for code-size reduction},
  author={Da Silva, Anderson Faustino and Kind, Bruno Conde and de Souza Magalh{\~a}es, Jos{\'e} Wesley and Rocha, Jer{\^o}nimo Nunes and Guimaraes, Breno Campos Ferreira and Pereira, Fernando Magno Quin{\~a}o},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={378--390},
  year={2021},
  organization={IEEE}
}
@article{iung2020systematic,
  title={Systematic mapping study on domain-specific language development tools},
  author={Iung, An{\'\i}bal and Carbonell, Jo{\~a}o and Marchezan, Luciano and Rodrigues, Elder and Bernardino, Maicon and Basso, Fabio Paulo and Medeiros, Bruno},
  journal={Empirical Software Engineering},
  volume={25},
  pages={4205--4249},
  year={2020},
  publisher={Springer}
}
@book{fowler2010domain,
  title={Domain-specific languages},
  author={Fowler, Martin},
  year={2010},
  publisher={Pearson Education}
}

@article{purini2013finding,
  title={Finding good optimization sequences covering program space},
  author={Purini, Suresh and Jain, Lakshya},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={4},
  pages={1--23},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{hong2025automated,
  title={Automated deep-learning model optimization framework for microcontrollers},
  author={Hong, Seungtae and Park, Gunju and Kim, Jeong-Si},
  journal={ETRI Journal},
  volume={47},
  number={2},
  pages={179--192},
  year={2025},
  publisher={Wiley Online Library}
}